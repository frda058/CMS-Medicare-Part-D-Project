{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer, log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_ftrs = ['Prscrbr_City',\n",
    "                    'Prscrbr_State_Abrvtn',\n",
    "                    'Brnd_Name',\n",
    "                    'Gnrc_Name']\n",
    "\n",
    "std_ftrs = ['Tot_Clms', \n",
    "            'Tot_30day_Fills', \n",
    "            'Tot_Day_Suply', \n",
    "            'Tot_Drug_Cst', \n",
    "            'Tot_Benes', \n",
    "            'GE65_Tot_Clms',\n",
    "            'GE65_Tot_30day_Fills',\n",
    "            'GE65_Tot_Drug_Cst',\n",
    "            'GE65_Tot_Day_Suply',\n",
    "            'GE65_Tot_Benes']\n",
    "\n",
    "\n",
    "\n",
    "#clf = Pipeline(steps=[('preprocessor', preprocessor)])                                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prscrbr_City</th>\n",
       "      <th>Prscrbr_State_Abrvtn</th>\n",
       "      <th>Brnd_Name</th>\n",
       "      <th>Gnrc_Name</th>\n",
       "      <th>Tot_Clms</th>\n",
       "      <th>Tot_30day_Fills</th>\n",
       "      <th>Tot_Day_Suply</th>\n",
       "      <th>Tot_Drug_Cst</th>\n",
       "      <th>Tot_Benes</th>\n",
       "      <th>GE65_Tot_Clms</th>\n",
       "      <th>GE65_Tot_30day_Fills</th>\n",
       "      <th>GE65_Tot_Drug_Cst</th>\n",
       "      <th>GE65_Tot_Day_Suply</th>\n",
       "      <th>GE65_Tot_Benes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>Aripiprazole</td>\n",
       "      <td>Aripiprazole</td>\n",
       "      <td>24</td>\n",
       "      <td>24.0</td>\n",
       "      <td>718</td>\n",
       "      <td>1198.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>Bupropion Xl</td>\n",
       "      <td>Bupropion Hcl</td>\n",
       "      <td>21</td>\n",
       "      <td>33.0</td>\n",
       "      <td>990</td>\n",
       "      <td>538.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>Clonazepam</td>\n",
       "      <td>Clonazepam</td>\n",
       "      <td>27</td>\n",
       "      <td>31.0</td>\n",
       "      <td>806</td>\n",
       "      <td>284.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>284.75</td>\n",
       "      <td>806.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>Clonidine Hcl</td>\n",
       "      <td>Clonidine Hcl</td>\n",
       "      <td>13</td>\n",
       "      <td>17.1</td>\n",
       "      <td>514</td>\n",
       "      <td>78.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA</td>\n",
       "      <td>Duloxetine Hcl</td>\n",
       "      <td>Duloxetine Hcl</td>\n",
       "      <td>13</td>\n",
       "      <td>13.0</td>\n",
       "      <td>390</td>\n",
       "      <td>477.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>Boca Raton</td>\n",
       "      <td>FL</td>\n",
       "      <td>Vyvanse</td>\n",
       "      <td>Lisdexamfetamine Dimesylate</td>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>540</td>\n",
       "      <td>5933.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4627</th>\n",
       "      <td>Boca Raton</td>\n",
       "      <td>FL</td>\n",
       "      <td>Ziprasidone Hcl</td>\n",
       "      <td>Ziprasidone Hcl</td>\n",
       "      <td>49</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1770</td>\n",
       "      <td>2619.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4628</th>\n",
       "      <td>Boca Raton</td>\n",
       "      <td>FL</td>\n",
       "      <td>Zolpidem Tartrate</td>\n",
       "      <td>Zolpidem Tartrate</td>\n",
       "      <td>158</td>\n",
       "      <td>183.0</td>\n",
       "      <td>5490</td>\n",
       "      <td>1199.12</td>\n",
       "      <td>22.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1057.17</td>\n",
       "      <td>4380.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4629</th>\n",
       "      <td>Tampa</td>\n",
       "      <td>FL</td>\n",
       "      <td>Estradiol</td>\n",
       "      <td>Estradiol</td>\n",
       "      <td>17</td>\n",
       "      <td>23.0</td>\n",
       "      <td>664</td>\n",
       "      <td>1930.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1930.07</td>\n",
       "      <td>664.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4630</th>\n",
       "      <td>Tampa</td>\n",
       "      <td>FL</td>\n",
       "      <td>Fluconazole</td>\n",
       "      <td>Fluconazole</td>\n",
       "      <td>17</td>\n",
       "      <td>17.0</td>\n",
       "      <td>80</td>\n",
       "      <td>65.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4631 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Prscrbr_City Prscrbr_State_Abrvtn          Brnd_Name  \\\n",
       "0         Seattle                   WA       Aripiprazole   \n",
       "1         Seattle                   WA       Bupropion Xl   \n",
       "2         Seattle                   WA         Clonazepam   \n",
       "3         Seattle                   WA      Clonidine Hcl   \n",
       "4         Seattle                   WA     Duloxetine Hcl   \n",
       "...           ...                  ...                ...   \n",
       "4626   Boca Raton                   FL            Vyvanse   \n",
       "4627   Boca Raton                   FL    Ziprasidone Hcl   \n",
       "4628   Boca Raton                   FL  Zolpidem Tartrate   \n",
       "4629        Tampa                   FL          Estradiol   \n",
       "4630        Tampa                   FL        Fluconazole   \n",
       "\n",
       "                        Gnrc_Name  Tot_Clms  Tot_30day_Fills  Tot_Day_Suply  \\\n",
       "0                    Aripiprazole        24             24.0            718   \n",
       "1                   Bupropion Hcl        21             33.0            990   \n",
       "2                      Clonazepam        27             31.0            806   \n",
       "3                   Clonidine Hcl        13             17.1            514   \n",
       "4                  Duloxetine Hcl        13             13.0            390   \n",
       "...                           ...       ...              ...            ...   \n",
       "4626  Lisdexamfetamine Dimesylate        18             18.0            540   \n",
       "4627              Ziprasidone Hcl        49             59.0           1770   \n",
       "4628            Zolpidem Tartrate       158            183.0           5490   \n",
       "4629                    Estradiol        17             23.0            664   \n",
       "4630                  Fluconazole        17             17.0             80   \n",
       "\n",
       "      Tot_Drug_Cst  Tot_Benes  GE65_Tot_Clms  GE65_Tot_30day_Fills  \\\n",
       "0          1198.86        NaN            0.0                   0.0   \n",
       "1           538.93        NaN            NaN                   NaN   \n",
       "2           284.75        NaN           27.0                  31.0   \n",
       "3            78.13        NaN            NaN                   NaN   \n",
       "4           477.59        NaN            NaN                   NaN   \n",
       "...            ...        ...            ...                   ...   \n",
       "4626       5933.52        NaN            0.0                   0.0   \n",
       "4627       2619.93        NaN            NaN                   NaN   \n",
       "4628       1199.12       22.0          127.0                 146.0   \n",
       "4629       1930.07        NaN           17.0                  23.0   \n",
       "4630         65.65        NaN            NaN                   NaN   \n",
       "\n",
       "      GE65_Tot_Drug_Cst  GE65_Tot_Day_Suply  GE65_Tot_Benes  \n",
       "0                  0.00                 0.0             NaN  \n",
       "1                   NaN                 NaN             NaN  \n",
       "2                284.75               806.0             NaN  \n",
       "3                   NaN                 NaN             NaN  \n",
       "4                   NaN                 NaN             NaN  \n",
       "...                 ...                 ...             ...  \n",
       "4626               0.00                 0.0             NaN  \n",
       "4627                NaN                 NaN             NaN  \n",
       "4628            1057.17              4380.0             NaN  \n",
       "4629            1930.07               664.0             NaN  \n",
       "4630                NaN                 NaN             NaN  \n",
       "\n",
       "[4631 rows x 14 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subsample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"xgbclassifier__subsample\": [0.5, 0.7, 0.9],\n",
    "              \"xgbclassifier__missing\": [np.nan],\n",
    "              \"xgbclassifier__max_depth\": [1, 3, 10],\n",
    "              \"xgbclassifier__learning_rate\": [0.001, 0.01, 0.1, 0.3],\n",
    "              \"xgbclassifier__n_estimators\": [1000],\n",
    "              \"xgbclassifier__gamma\": [1,5,10]}\n",
    "\n",
    "\n",
    "param_grid1 = {\n",
    "              \"xgbclassifier__missing\": [np.nan],\n",
    "              \"xgbclassifier__learning_rate\": [0.2],\n",
    "              \"xgbclassifier__max_depth\": [15],\n",
    "              \"xgbclassifier__gamma\": [5],\n",
    "              \"xgbclassifier__n_estimators\": [300]}\n",
    "\n",
    "\n",
    "fit_params = {\"xgbclassifier__early_stopping_rounds\": 50}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_3specialties_equalWeight_subsample.zip',compression='zip', index_col=False)\n",
    "y = pd.read_csv('y_3specialties_equalWeight_subsample.zip',compression='zip')\n",
    "groups = pd.read_csv('groups_3specialties_equalWeight_subsample.zip',compression='zip')\n",
    "\n",
    "X = X.iloc[:,1:]\n",
    "y = y.iloc[:,1:]\n",
    "groups = groups.iloc[:,1:]\n",
    "\n",
    "y_columns = y.columns\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = y.values.ravel()\n",
    "y = le.fit_transform(y)\n",
    "y = pd.DataFrame(y)\n",
    "y.columns = y_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5025, 14)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with GridSearchCV (no early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_pipeline_groups_GridSearchCV(X,y,groups,random_state,n_folds):\n",
    "    # create a test set based on groups\n",
    "    splitter = GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=random_state)\n",
    "    \n",
    "    for i_other,i_test in splitter.split(X, y, groups):\n",
    "        X_other, y_other, groups_other = X.iloc[i_other], y.iloc[i_other], groups.iloc[i_other]\n",
    "        X_test, y_test, groups_test = X.iloc[i_test], y.iloc[i_test], groups.iloc[i_test]\n",
    "        \n",
    "\n",
    "    # check the split\n",
    "#     print(pd.unique(groups))\n",
    "#     print(pd.unique(groups_other))\n",
    "#     print(pd.unique(groups_test))\n",
    "    # splitter for _other\n",
    "    kf = GroupKFold(n_splits=n_folds)\n",
    "    # create the pipeline: preprocessor + supervised ML method\n",
    "\n",
    "    \n",
    "    clf = xgb.XGBClassifier(num_class=3,\n",
    "                                eval_metric = \"mlogloss\",\n",
    "                                objective = \"multi:softprob\",\n",
    "                                random_state = random_state, \n",
    "                                use_label_encoder = False)\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), categorical_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "    \n",
    "    pipe = make_pipeline(preprocessor,clf)\n",
    "    \n",
    "    # the parameter(s) we want to tune\n",
    "    param_grid = {\"xgbclassifier__subsample\": [0.5, 0.7],\n",
    "              \"xgbclassifier__missing\": [np.nan],\n",
    "              \"xgbclassifier__max_depth\": [1, 3, 10, 30],\n",
    "              \"xgbclassifier__learning_rate\": [0.01, 0.1, 0.3],\n",
    "              \"xgbclassifier__n_estimators\": [300],\n",
    "              #\"xgbclassifier__gamma\": [1,5,10],\n",
    "              \"xgbclassifier__reg_alpha\" : [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              \"xgbclassifier__reg_lambda\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "              \"xgbclassifier__seed\": [random_state]  }\n",
    "    \n",
    "    # prepare gridsearch\n",
    "    #grid = GridSearchCV(pipe, param_grid=param_grid,scoring = make_scorer(accuracy_score),\n",
    "                        #cv=kf, return_train_score = True)\n",
    "    \n",
    "    #LogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True, labels = sorted(np.unique(y)))\n",
    "    \n",
    "    grid = GridSearchCV(pipe, \n",
    "                        param_grid=param_grid1,\n",
    "                        scoring = \"f1_macro\",\n",
    "                        cv=kf, \n",
    "                        return_train_score = True, \n",
    "                        n_jobs=1, \n",
    "                        verbose=10)\n",
    "    \n",
    "    # do kfold CV on _other\n",
    "    grid_result = grid.fit(X_other, y_other, groups=groups_other)\n",
    "    \n",
    "    print()\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    \n",
    "    print(f'Best params: {grid.best_params_}')\n",
    "    \n",
    "    print(f\"mean CV: {means} +/ {stds}\")\n",
    "    \n",
    "    y_test_pred_proba = grid.predict_proba(X_test)\n",
    "    \n",
    "    y_test_pred = grid.predict(X_test)\n",
    "    \n",
    "    #score = accuracy_score(y_test,y_test_pred)\n",
    "    \n",
    "    #score = f1_score(y_test,y_test_pred, average = \"macro\")\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    #confusion_mat.append(cm)\n",
    "    \n",
    "    # get the feature importances\n",
    "    metrics = [\"weight\", \"gain\", \"cover\", \"total_gain\", \"total_cover\"]\n",
    "    importances = []\n",
    "\n",
    "    for i in metrics:\n",
    "        booster_score = clf.best_estimator_.get_booster().get_score(importance_type='gain')\n",
    "        importances.append(booster_score)\n",
    "        #.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "    \n",
    "    \n",
    "    #logloss = log_loss(y_test, y_test_pred_proba,labels = sorted(np.unique(y)))\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    f_05_score = fbeta_score(y_test, y_test_pred, beta = 0.5, labels=sorted(np.unique(y)), average='macro')\n",
    "    \n",
    "    print(f'F05: {f_05_score}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return f_05_score, grid.best_params_, cm, importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = []\n",
    "best_params = []\n",
    "confusion_mat = []\n",
    "xgboost_metrics = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State # 0\n",
      "\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100\n",
      "[CV 1/5; 1/1] END xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100;, score=(train=0.972, test=0.739) total time=  23.9s\n",
      "[CV 2/5; 1/1] START xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100\n",
      "[CV 2/5; 1/1] END xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100;, score=(train=0.965, test=0.530) total time=  25.7s\n",
      "[CV 3/5; 1/1] START xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100\n",
      "[CV 3/5; 1/1] END xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100;, score=(train=0.983, test=0.514) total time=  28.0s\n",
      "[CV 4/5; 1/1] START xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100\n",
      "[CV 4/5; 1/1] END xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100;, score=(train=0.981, test=0.337) total time=  28.8s\n",
      "[CV 5/5; 1/1] START xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100\n",
      "[CV 5/5; 1/1] END xgbclassifier__gamma=5, xgbclassifier__learning_rate=0.2, xgbclassifier__max_depth=15, xgbclassifier__missing=nan, xgbclassifier__n_estimators=100;, score=(train=0.962, test=0.628) total time=  27.4s\n",
      "Best params: {'xgbclassifier__gamma': 5, 'xgbclassifier__learning_rate': 0.2, 'xgbclassifier__max_depth': 15, 'xgbclassifier__missing': nan, 'xgbclassifier__n_estimators': 100}\n",
      "mean CV: [0.54926907] +/ [0.13318022]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'clf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:11\u001b[0m\n",
      "Cell \u001b[0;32mIn [199], line 83\u001b[0m, in \u001b[0;36mML_pipeline_groups_GridSearchCV\u001b[0;34m(X, y, groups, random_state, n_folds)\u001b[0m\n\u001b[1;32m     80\u001b[0m importances \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[0;32m---> 83\u001b[0m     booster_score \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mget_booster()\u001b[38;5;241m.\u001b[39mget_score(importance_type \u001b[38;5;241m=\u001b[39m i)\n\u001b[1;32m     84\u001b[0m     importances\u001b[38;5;241m.\u001b[39mappend(booster_score)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m#.get_booster().get_score(importance_type='weight')\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m#logloss = log_loss(y_test, y_test_pred_proba,labels = sorted(np.unique(y)))\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clf'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_test_pred_prob_list = []\n",
    "\n",
    "logloss_list = []\n",
    "\n",
    "nr_states = [1,2,3]\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'Random State # {i}')\n",
    "    print()\n",
    "    \n",
    "    score, bestparams, cmat, importances = ML_pipeline_groups_GridSearchCV(X, y, groups, i*42, 5)\n",
    "    \n",
    "    #logloss_list.append(result)\n",
    "    \n",
    "   \n",
    "    test_scores.append(score)\n",
    "    best_params.append(bestparams)\n",
    "    confusion_mat.append(cmat)\n",
    "    xgboost_metrics.append(importances)\n",
    "    #y_test_pred_prob_list.append(y_test_pred_prob)\n",
    "    \n",
    "    #print()\n",
    "\n",
    "    \n",
    "    #best_params.append(grid.best_params_)\n",
    "    #print()\n",
    "    #print('best CV score:',grid.best_score_)\n",
    "    #print()\n",
    "    #print('test score:', score)\n",
    "    \n",
    "    #print()\n",
    "    \n",
    "#print('test logloss:',np.around(np.mean(logloss_list),2),'+/-',np.around(np.std(logloss_list),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost with ParamdGrid Iteration (WITH early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_model(X_train, Y_train, X_CV, y_CV, X_test, y_test, i, verbose=4):\n",
    "\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    Y_train = np.reshape(np.array(Y_train), (1, -1)).ravel()\n",
    "    y_CV = np.reshape(np.array(y_CV), (1, -1)).ravel()\n",
    "    y_test = np.reshape(np.array(y_test), (1, -1)).ravel()\n",
    "\n",
    "    \n",
    "    XGB = xgb.XGBClassifier(num_class=3,\n",
    "                      eval_metric = \"mlogloss\",\n",
    "                      objective = \"multi:softprob\",\n",
    "                      random_state = i, \n",
    "                      use_label_encoder = False)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {\"learning_rate\": [0.01, 0.1, 0.3],#0.01,\n",
    "                  \"n_estimators\": [10000],\n",
    "                  \"seed\": [i],\n",
    "                  \"reg_alpha\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "                  #\"reg_lambda\": [0e0, 1e-2, 1e-1, 1e0, 1e1, 1e2],\n",
    "                  \"missing\": [np.nan], \n",
    "                  \"max_depth\": [1, 3, 10], #1,3,\n",
    "                  \"colsample_bytree\": [0.5, 0.7, 0.9],              \n",
    "                  \"subsample\": [0.5, 0.7, 0.9]} #, 0.9\n",
    "    \n",
    "    \n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros(len(pg))\n",
    "\n",
    "    #weights = compute_sample_weight(class_weight='balanced', y = Y_train)\n",
    "    \n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "            print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        XGB.set_params(**params)\n",
    "        eval_set = [(X_CV, y_CV)]\n",
    "        XGB.fit(X_train, Y_train,\n",
    "                early_stopping_rounds = 50, eval_set=eval_set, verbose=False)\n",
    "                #sample_weight = weights)\n",
    "            \n",
    "        #y_CV_pred_proba = XGB.predict_proba(X_CV, iteration_range=(0, XGB.best_ntree_limit))\n",
    "        y_CV_pred = XGB.predict(X_CV, iteration_range=(0, XGB.best_ntree_limit))\n",
    "        #scores[i] = accuracy_score(y_CV,y_CV_pred)\n",
    "        scores[i] = fbeta_score(y_CV, y_CV_pred, beta = 0.5, labels=sorted(np.unique(y)), average='macro')\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    \n",
    "    if verbose >= 4:\n",
    "        print(f'Validation set best score {np.max(scores)}')\n",
    "        print()\n",
    "        print(f'Validation set best params {best_params}')\n",
    "        print()\n",
    "        \n",
    "    # test the model on the test set with best parameter set\n",
    "    XGB.set_params(**best_params[0])\n",
    "    \n",
    "    XGB.fit(X_train,Y_train,\n",
    "            early_stopping_rounds=50, eval_set=eval_set, verbose=False)\n",
    "    \n",
    "    y_test_pred = XGB.predict(X_test, iteration_range=(0, XGB.best_ntree_limit))\n",
    "    \n",
    "    #test_logloss = log_loss(y_test, y_test_pred_proba)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The Test F0.5 is:', fbeta_score(y_test, y_test_pred, beta = 0.5, labels=sorted(np.unique(y)), average='macro')) \n",
    "        print()\n",
    "        \n",
    "    if verbose >= 2:\n",
    "        #print ('The predictions are:')\n",
    "        #print (y_test_pred)\n",
    "        print()\n",
    "    #if verbose >= 3:\n",
    "        #print(\"Feature importances:\")\n",
    "        #print(XGB.feature_importances_)\n",
    "    \n",
    "    return (fbeta_score(y_test, y_test_pred, beta = 0.5, labels=sorted(np.unique(y)), average='macro')\n",
    "            , y_test_pred, XGB.feature_importances_)\n",
    "    #return (y_test_pred, best_params, XGB.feature_importances_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_pipeline_XGBoost_EarlyStop(X,y,groups,random_state,n_folds):\n",
    "    \n",
    "    \n",
    "    result_by_fold = []\n",
    "    \n",
    "    prediction_by_fold = []\n",
    "    \n",
    "    feat_impo_by_fold = []\n",
    "    \n",
    "    # create a test set based on groups\n",
    "    splitter = GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=random_state)\n",
    "    \n",
    "    for i_other,i_test in splitter.split(X, y, groups):\n",
    "        X_other, y_other, groups_other = X.iloc[i_other], y.iloc[i_other], groups.iloc[i_other]\n",
    "        X_test, y_test, groups_test = X.iloc[i_test], y.iloc[i_test], groups.iloc[i_test]\n",
    "        \n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=i)\n",
    "    \n",
    "    # Get Test Set\n",
    "    for i_other,i_test in splitter.split(X, y, groups):\n",
    "        X_other, y_other, groups_other = X.iloc[i_other], y.iloc[i_other], groups.iloc[i_other]\n",
    "        X_test, y_test, groups_test = X.iloc[i_test], y.iloc[i_test], groups.iloc[i_test]\n",
    "    \n",
    "    # Get Validation Set\n",
    "    \n",
    "    kf = GroupKFold(n_splits=n_folds)\n",
    "    counter = 0\n",
    "    \n",
    "    for i_train, i_test in kf.split(X_other, y_other, groups_other):\n",
    "        X_train, y_train, groups_train = X_other.iloc[i_train], y_other.iloc[i_train], groups_other.iloc[i_train]\n",
    "        X_val, y_val, groups_val = X_other.iloc[i_test], y_other.iloc[i_test], groups_other.iloc[i_test]\n",
    "        \n",
    "        #print(len(y_val))\n",
    "        #print(len(y_train))\n",
    "        counter = counter + 1\n",
    "        \n",
    "        print(f\"CV # {counter}\")\n",
    "        X_prep = preprocessor.fit_transform(X_train)\n",
    "        # collect feature names\n",
    "        \n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        \n",
    "        df_train = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "        print(f\"Train Set Shape (after preprocessing): {df_train.shape}\")\n",
    "        print()\n",
    "        \n",
    "        # transform the CV\n",
    "        df_CV = preprocessor.transform(X_val)\n",
    "        df_CV = pd.DataFrame(data=df_CV,columns = feature_names)\n",
    "        print(f\"CV Set Shape (after preprocessing): {df_CV.shape}\")\n",
    "        print()\n",
    "        # transform the test\n",
    "        df_test = preprocessor.transform(X_test)\n",
    "        df_test = pd.DataFrame(data=df_test,columns = feature_names)\n",
    "        print(f\"Test Set Shape (after preprocessing): {df_test.shape}\")\n",
    "        print()\n",
    "        \n",
    "        y_CV = y_val\n",
    "        \n",
    "        \n",
    "        # run XGB\n",
    "        result, prediction, featimpo = xgb_model(df_train, y_train, df_CV, y_CV, df_test, y_test, i, verbose=4)\n",
    "        \n",
    "        #feat_impo.append(sub_y_test_pred[2])\n",
    "        \n",
    "        #sub_y_test_pred = pd.DataFrame(sub_y_test_pred[1],columns=['y_test_pred'])\n",
    "        \n",
    "    \n",
    "        result_by_fold.append(result)\n",
    "        \n",
    "        prediction_by_fold.append(prediction)\n",
    "        \n",
    "        feat_impo_by_fold.append(featimpo)\n",
    "\n",
    "        # get global scores\n",
    "        #total_accuracy = (accuracy_score(y_test,sub_y_test_pred))\n",
    "        #f1 = f1_score(y_test,sub_y_test_pred, average = 'macro')\n",
    "\n",
    "        #cm = confusion_matrix(y_test,sub_y_test_pred)\n",
    "        \n",
    "        #print()\n",
    "        #print('   Test Accuracy:', accuracy_score(y_test,sub_y_test_pred))\n",
    "        #print('   Test F1 Score:', f1_score(y_test, sub_y_test_pred, average = 'macro'))\n",
    "\n",
    "        \n",
    "    return (result_by_fold, prediction_by_fold, feat_impo_by_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y_test_pred = []\n",
    "feat_impo_list = []\n",
    "test_scores = []\n",
    "best_params = []\n",
    "confusion_mat = []\n",
    "class_met = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State # 0\n",
      "\n",
      "CV # 1\n",
      "Train Set Shape (after preprocessing): (3198, 906)\n",
      "\n",
      "CV Set Shape (after preprocessing): (800, 906)\n",
      "\n",
      "Test Set Shape (after preprocessing): (1027, 906)\n",
      "\n",
      "Validation set best score and parameters are:\n",
      "0.7036231770712799\n",
      "[{'colsample_bytree': 0.7, 'learning_rate': 0.03, 'max_depth': 3, 'missing': nan, 'n_estimators': 10000, 'seed': 0, 'subsample': 0.7}]\n",
      "\n",
      "The Test F0.5 is: 0.6851974127939119\n",
      "\n",
      "\n",
      "CV # 2\n",
      "Train Set Shape (after preprocessing): (3198, 918)\n",
      "\n",
      "CV Set Shape (after preprocessing): (800, 918)\n",
      "\n",
      "Test Set Shape (after preprocessing): (1027, 918)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "Cell \u001b[0;32mIn [175], line 63\u001b[0m, in \u001b[0;36mML_pipeline_XGBoost_EarlyStop\u001b[0;34m(X, y, groups, random_state, n_folds)\u001b[0m\n\u001b[1;32m     59\u001b[0m y_CV \u001b[38;5;241m=\u001b[39m y_val\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# run XGB\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m result, prediction, featimpo \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_CV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_CV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#feat_impo.append(sub_y_test_pred[2])\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#sub_y_test_pred = pd.DataFrame(sub_y_test_pred[1],columns=['y_test_pred'])\u001b[39;00m\n\u001b[1;32m     70\u001b[0m result_by_fold\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn [171], line 38\u001b[0m, in \u001b[0;36mxgb_model\u001b[0;34m(X_train, Y_train, X_CV, y_CV, X_test, y_test, i, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m XGB\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m     37\u001b[0m eval_set \u001b[38;5;241m=\u001b[39m [(X_CV, y_CV)]\n\u001b[0;32m---> 38\u001b[0m \u001b[43mXGB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;66;03m#sample_weight = weights)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#y_CV_pred_proba = XGB.predict_proba(X_CV, iteration_range=(0, XGB.best_ntree_limit))\u001b[39;00m\n\u001b[1;32m     43\u001b[0m y_CV_pred \u001b[38;5;241m=\u001b[39m XGB\u001b[38;5;241m.\u001b[39mpredict(X_CV, iteration_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, XGB\u001b[38;5;241m.\u001b[39mbest_ntree_limit))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/core.py:506\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    505\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/sklearn.py:1250\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1230\u001b[0m model, feval, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, eval_metric, params)\n\u001b[1;32m   1231\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1232\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1233\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     label_transform\u001b[38;5;241m=\u001b[39mlabel_transform,\n\u001b[1;32m   1248\u001b[0m )\n\u001b[0;32m-> 1250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/training.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(params, dtrain, num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, evals\u001b[38;5;241m=\u001b[39m(), obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m           maximize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, evals_result\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    117\u001b[0m           verbose_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xgb_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124;03m\"\"\"Train a booster with given parameters.\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    Booster : a trained booster model\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 188\u001b[0m     bst \u001b[38;5;241m=\u001b[39m \u001b[43m_train_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bst\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/training.py:81\u001b[0m, in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/core.py:1680\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1680\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_scores = []\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'Random State # {i}')\n",
    "    print()\n",
    "    \n",
    "    result, prediction, featimpo = ML_pipeline_XGBoost_EarlyStop(X, y, groups, i*42, 5)\n",
    "    \n",
    "    #confusion_mat.append(cm)\n",
    "    \n",
    "    #feat_impo_list.append(feat_impo)\n",
    "    \n",
    "    #class_met.append(class_metrics)\n",
    "    print()\n",
    "    #print(f'Best params: {grid.best_params_}')\n",
    "    \n",
    "    #best_params.append(grid.best_params_)\n",
    "    #print()\n",
    "    #print('best CV score:',grid.best_score_)\n",
    "    #print()\n",
    "    #print('test score:', score)\n",
    "    test_scores.append(result)\n",
    "    print()\n",
    "    \n",
    "print('test accuracy:',np.around(np.mean(test_scores),2),'+/-',np.around(np.std(test_scores),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
